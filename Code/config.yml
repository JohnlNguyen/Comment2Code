transformer:
  embed_dim: 256
  hidden_dim: 256
  ff_dim: 2048
  num_layers: 4
  attention_dim: 512
  num_heads: 8
  dropout_rate: 0.1
  warmup: 4000
  base_lr: 2.0
baseline:
  embed_dim: 512
  hidden_dim: 512
  num_layers: 2
  dropout_rate: 0.5
data:
  batch_size: 32
  max_batch_size: 3000
  max_sample_size: 100
  percent: 1.0
vocabulary:
  vocab_cutoff: 10
  split_tokens: "bpe"
  split_file: true
  bpe_limit: 5000
  tokenizer: "subword"
training:
  num_epochs: 10
  print_freq: 5
  model: "code_transformer"
  lr: 0.0001
  beta: .5
  loss_function: "log_loss"
  log_dir: "../data/logs/java"
  input_type: "code"
  checkpoint: "./checkpoints/code"
  save_valid:
